{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch: A Pedagogical Implementation\n",
    "\n",
    "This notebook provides a comprehensive, step-by-step walkthrough of building, training, and understanding neural networks from first principles using NumPy.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand the mathematical foundations of neural networks\n",
    "2. Implement forward and backward propagation from scratch\n",
    "3. Train networks on nonlinearly separable problems (XOR)\n",
    "4. Validate implementations through gradient checking\n",
    "5. Compare with PyTorch implementations\n",
    "\n",
    "**Prerequisites:** Basic linear algebra, calculus, and Python programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.models import OneHiddenLayerMLP\n",
    "from src.activations import ReLU, Sigmoid\n",
    "from src.data import generate_xor_data, generate_spiral_data\n",
    "from src.optimization import SGD, train, evaluate\n",
    "from src.gradient_check import check_all_gradients\n",
    "from src.visualization import plot_training_history, plot_decision_boundary, plot_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundations\n",
    "\n",
    "### 1.1 Network Architecture\n",
    "\n",
    "We implement a **one-hidden-layer Multi-Layer Perceptron (MLP)**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_1 &= W_1 x + b_1 \\in \\mathbb{R}^h \\quad \\text{(first affine)} \\\\\n",
    "h_1 &= \\varphi(a_1) \\in \\mathbb{R}^h \\quad \\text{(activation)} \\\\\n",
    "f &= W_2 h_1 + b_2 \\in \\mathbb{R} \\quad \\text{(output)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x \\in \\mathbb{R}^d$: input\n",
    "- $W_1 \\in \\mathbb{R}^{h \\times d}$, $b_1 \\in \\mathbb{R}^h$: first layer parameters\n",
    "- $\\varphi$: activation function (ReLU or sigmoid)\n",
    "- $W_2 \\in \\mathbb{R}^{1 \\times h}$, $b_2 \\in \\mathbb{R}$: second layer parameters\n",
    "\n",
    "### 1.2 Loss Function\n",
    "\n",
    "We use **squared error loss**:\n",
    "\n",
    "$$L = \\frac{1}{2}(f - y)^2$$\n",
    "\n",
    "The factor $\\frac{1}{2}$ simplifies the gradient: $\\frac{\\partial L}{\\partial f} = f - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity**, enabling networks to learn complex patterns.\n",
    "\n",
    "### 2.1 ReLU (Rectified Linear Unit)\n",
    "\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "$$\\text{ReLU}'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU\n",
    "z = np.linspace(-5, 5, 200)\n",
    "relu = ReLU()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Forward\n",
    "ax1.plot(z, relu.forward(z), 'b-', linewidth=2)\n",
    "ax1.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.set_xlabel('z', fontsize=12)\n",
    "ax1.set_ylabel('ReLU(z)', fontsize=12)\n",
    "ax1.set_title('ReLU Forward', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Derivative\n",
    "ax2.plot(z, relu.derivative(z), 'r-', linewidth=2)\n",
    "ax2.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.set_xlabel('z', fontsize=12)\n",
    "ax2.set_ylabel(\"ReLU'(z)\", fontsize=12)\n",
    "ax2.set_title('ReLU Derivative', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ReLU is piecewise linear: zero for negative inputs, identity for positive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sigmoid\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sigmoid\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Forward\n",
    "ax1.plot(z, sigmoid.forward(z), 'b-', linewidth=2)\n",
    "ax1.axhline(0.5, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.set_xlabel('z', fontsize=12)\n",
    "ax1.set_ylabel('σ(z)', fontsize=12)\n",
    "ax1.set_title('Sigmoid Forward', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Derivative\n",
    "ax2.plot(z, sigmoid.derivative(z), 'r-', linewidth=2)\n",
    "ax2.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax2.set_xlabel('z', fontsize=12)\n",
    "ax2.set_ylabel(\"σ'(z)\", fontsize=12)\n",
    "ax2.set_title('Sigmoid Derivative', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid squashes inputs to (0, 1), useful for probabilistic outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Forward Propagation\n",
    "\n",
    "### 3.1 Create and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "model = OneHiddenLayerMLP(input_dim=2, hidden_dim=4, activation='relu')\n",
    "print(model)\n",
    "\n",
    "# Inspect parameters\n",
    "params = model.get_parameters()\n",
    "print(f\"\\nParameter shapes:\")\n",
    "for name, param in params.items():\n",
    "    print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Forward Pass on Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample forward pass\n",
    "x = np.array([1.0, -0.5])\n",
    "f = model.forward(x)\n",
    "\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"\\nIntermediate values:\")\n",
    "print(f\"  a₁ (pre-activation): {model.a1_cache}\")\n",
    "print(f\"  h₁ (post-activation): {model.h1_cache}\")\n",
    "print(f\"\\nOutput f: {f:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Backpropagation\n",
    "\n",
    "Backpropagation computes gradients using the **chain rule**:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial f} \\cdot \\frac{\\partial f}{\\partial \\theta}$$\n",
    "\n",
    "### 4.1 Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "y = 1.0  # Target\n",
    "grads = model.backward(x, y, f)\n",
    "\n",
    "print(f\"Target y: {y}\")\n",
    "print(f\"Prediction f: {f:.6f}\")\n",
    "print(f\"Loss: {0.5 * (f - y)**2:.6f}\")\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "for name, grad in grads.items():\n",
    "    print(f\"  {name}: shape {grad.shape}, mean {np.mean(np.abs(grad)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Checking\n",
    "\n",
    "Verify analytical gradients match numerical gradients computed via finite differences:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta_i} \\approx \\frac{L(\\theta + \\epsilon e_i) - L(\\theta - \\epsilon e_i)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all gradients\n",
    "results = check_all_gradients(model, x, y, epsilon=1e-5, threshold=1e-7)\n",
    "\n",
    "print(\"Gradient Checking Results:\\n\")\n",
    "for param_name, result in results.items():\n",
    "    status = \"✓ PASS\" if result['passed'] else \"✗ FAIL\"\n",
    "    print(f\"{param_name}: {status} (error: {result['error']:.2e})\")\n",
    "\n",
    "if all(r['passed'] for r in results.values()):\n",
    "    print(\"\\n✓ All gradients verified correct!\")\n",
    "else:\n",
    "    print(\"\\n✗ Some gradients failed verification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The XOR Problem\n",
    "\n",
    "XOR is a classic **nonlinearly separable** problem that cannot be solved by linear models.\n",
    "\n",
    "### 5.1 Generate XOR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR dataset\n",
    "X_train, y_train = generate_xor_data(n_samples=200, noise=0.1)\n",
    "X_val, y_val = generate_xor_data(n_samples=100, noise=0.1)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "\n",
    "# Visualize XOR data\n",
    "plot_dataset(X_train, y_train)\n",
    "plt.title('XOR Problem: Nonlinearly Separable', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"  Class 0: {np.sum(y_train == 0)} samples\")\n",
    "print(f\"  Class 1: {np.sum(y_train == 1)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Model on XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "xor_model = OneHiddenLayerMLP(input_dim=2, hidden_dim=8, activation='relu')\n",
    "optimizer = SGD(learning_rate=0.5)\n",
    "\n",
    "print(\"Training on XOR...\\n\")\n",
    "history = train(\n",
    "    xor_model, X_train, y_train, optimizer,\n",
    "    n_epochs=200,\n",
    "    batch_size=32,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    verbose=True,\n",
    "    print_every=50\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_history(history)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final validation loss: {history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_metrics = evaluate(xor_model, X_val, y_val, threshold=0.5)\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"  Loss: {val_metrics['loss']:.6f}\")\n",
    "print(f\"  Accuracy: {val_metrics['accuracy']:.2%}\")\n",
    "\n",
    "if val_metrics['accuracy'] > 0.85:\n",
    "    print(\"\\n✓ Model successfully learned XOR pattern!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Model needs more training or hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "plot_decision_boundary(xor_model, X_val, y_val, resolution=200)\n",
    "plt.title('Learned Decision Boundary for XOR', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"The network has learned to separate the XOR classes using a nonlinear boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: PyTorch Comparison\n",
    "\n",
    "Let's verify our implementation matches PyTorch's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.pytorch_models import ModuleMLP, train_pytorch_model, convert_numpy_to_pytorch\n",
    "\n",
    "# Create PyTorch model\n",
    "pt_model = ModuleMLP(input_dim=2, hidden_dim=8, activation='relu')\n",
    "\n",
    "# Convert NumPy data to PyTorch tensors\n",
    "X_train_pt = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train_pt = torch.from_numpy(y_train.astype(np.float32))\n",
    "\n",
    "# Train PyTorch model\n",
    "print(\"Training PyTorch model...\")\n",
    "pt_history = train_pytorch_model(\n",
    "    pt_model, X_train_pt, y_train_pt,\n",
    "    n_epochs=200,\n",
    "    learning_rate=0.5,\n",
    "    batch_size=32,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    X_val_pt = torch.from_numpy(X_val.astype(np.float32))\n",
    "    pt_predictions = pt_model.forward(X_val_pt).numpy()\n",
    "    pt_accuracy = np.mean((pt_predictions >= 0.5).astype(int) == y_val)\n",
    "\n",
    "print(f\"\\nPyTorch Model Accuracy: {pt_accuracy:.2%}\")\n",
    "print(f\"NumPy Model Accuracy: {val_metrics['accuracy']:.2%}\")\n",
    "print(\"\\n✓ Both implementations achieve similar performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Appendix A Worked Example\n",
    "\n",
    "Let's verify the exact numerical example from the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for Appendix A example\n",
    "appendix_model = OneHiddenLayerMLP(input_dim=2, hidden_dim=2, activation='relu')\n",
    "\n",
    "# Set specific parameters from Appendix A\n",
    "appendix_model.set_parameters({\n",
    "    'W1': np.array([[1.0, -2.0], [0.5, 1.0]]),\n",
    "    'b1': np.array([0.0, 0.0]),\n",
    "    'W2': np.array([[1.0, -1.0]]),\n",
    "    'b2': np.array([0.0])\n",
    "})\n",
    "\n",
    "# Input and target from Appendix A\n",
    "x_app = np.array([1.0, -1.0])\n",
    "y_app = 2.0\n",
    "\n",
    "# Forward pass\n",
    "f_app = appendix_model.forward(x_app)\n",
    "L_app = 0.5 * (f_app - y_app) ** 2\n",
    "\n",
    "print(\"Appendix A Example:\")\n",
    "print(f\"  a₁ = {appendix_model.a1_cache}  (expected: [3.0, -0.5])\")\n",
    "print(f\"  h₁ = {appendix_model.h1_cache}  (expected: [3.0, 0.0])\")\n",
    "print(f\"  f  = {f_app:.6f}  (expected: 3.0)\")\n",
    "print(f\"  L  = {L_app:.6f}  (expected: 0.5)\")\n",
    "\n",
    "# Backward pass\n",
    "grads_app = appendix_model.backward(x_app, y_app, f_app)\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"  ∂L/∂W₂ = {grads_app['dL_dW2']}\")\n",
    "print(f\"           (expected: [[3.0, 0.0]])\")\n",
    "print(f\"  ∂L/∂b₂ = {grads_app['dL_db2']}\")\n",
    "print(f\"           (expected: [1.0])\")\n",
    "print(f\"  ∂L/∂W₁ = {grads_app['dL_dW1']}\")\n",
    "print(f\"           (expected: [[1.0, -1.0], [0.0, 0.0]])\")\n",
    "print(f\"  ∂L/∂b₁ = {grads_app['dL_db1']}\")\n",
    "print(f\"           (expected: [1.0, 0.0])\")\n",
    "\n",
    "# Verify all match\n",
    "assert np.allclose(appendix_model.a1_cache, [3.0, -0.5], rtol=1e-10)\n",
    "assert np.allclose(appendix_model.h1_cache, [3.0, 0.0], rtol=1e-10)\n",
    "assert np.isclose(f_app, 3.0, rtol=1e-10)\n",
    "print(\"\\n✓ All values match Appendix A specification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Forward Propagation**: Computing outputs through layers\n",
    "   - Affine transformations: $z = Wx + b$\n",
    "   - Nonlinear activations: ReLU, Sigmoid\n",
    "\n",
    "2. **Backpropagation**: Computing gradients via chain rule\n",
    "   - Analytical gradients match numerical gradients\n",
    "   - Gradient checking validates implementation\n",
    "\n",
    "3. **Training**: Optimizing parameters with SGD\n",
    "   - Mini-batch gradient descent\n",
    "   - Learning rate selection\n",
    "   - Monitoring training progress\n",
    "\n",
    "4. **Nonlinear Problems**: XOR demonstrates necessity of hidden layers\n",
    "   - Linear models cannot solve XOR\n",
    "   - Single hidden layer with ReLU succeeds\n",
    "\n",
    "5. **Implementation Equivalence**: NumPy ↔ PyTorch\n",
    "   - Same mathematical operations\n",
    "   - PyTorch provides automation and GPU acceleration\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Deeper Networks**: Multiple hidden layers\n",
    "- **Advanced Optimizers**: Adam, RMSprop, momentum\n",
    "- **Regularization**: L2, dropout, batch normalization\n",
    "- **Different Architectures**: CNNs, RNNs, Transformers\n",
    "\n",
    "### References:\n",
    "\n",
    "- *Deep Learning* by Goodfellow, Bengio, and Courville\n",
    "- *Neural Networks and Deep Learning* by Michael Nielsen\n",
    "- PyTorch documentation: https://pytorch.org/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Experiment with Architecture**:\n",
    "   - Try different hidden layer sizes (2, 4, 8, 16)\n",
    "   - Compare ReLU vs Sigmoid activation\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Test learning rates: 0.01, 0.1, 1.0\n",
    "   - Try different batch sizes\n",
    "\n",
    "3. **Data Exploration**:\n",
    "   - Train on spiral data: `generate_spiral_data(n_samples=300, n_classes=3)`\n",
    "   - Visualize the decision boundaries\n",
    "\n",
    "4. **Implementation Extensions**:\n",
    "   - Add momentum to SGD\n",
    "   - Implement learning rate decay\n",
    "   - Add L2 regularization\n",
    "\n",
    "5. **Analysis**:\n",
    "   - Plot loss surface for different parameters\n",
    "   - Analyze gradient magnitudes during training\n",
    "   - Compare convergence speeds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
